{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e4d158",
   "metadata": {},
   "source": [
    "Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a543f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26912867",
   "metadata": {},
   "source": [
    "Generate Distribution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08e76197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sorted_indexes(payoffs):\n",
    "    vals_indexes=[]\n",
    "    ind_by_val = []\n",
    "\n",
    "    for i in range(len(payoffs)):\n",
    "        vals_indexes.append([payoffs[i],i])\n",
    "    \n",
    "    vals_indexes.sort(reverse=True)\n",
    "    for x in vals_indexes:\n",
    "        ind_by_val.append(x[1])\n",
    "    return ind_by_val\n",
    "\n",
    "def find_min_index(payoffs):\n",
    "    min_value = min(payoffs)\n",
    "    min_index = payoffs.index(min_value)\n",
    "    return min_index\n",
    "\n",
    "\n",
    "def generate_adversarial_payoffs(num_actions, num_rounds):\n",
    "    rounds_list = []\n",
    "    totals_by_round = []\n",
    "    #first_payoffs = [round(random.random(),2) for i in range(num_actions)]\n",
    "    initial_payoff = round(random.random(), 2)\n",
    "    first_payoffs = [0 for i in range(num_actions)]\n",
    "    first_payoffs[random.randrange(num_actions)] = initial_payoff\n",
    "    total_payoffs = [first_payoffs[i] for i in range(num_actions)]\n",
    "    #total_indexes = sorted_indexes(total_payoffs)\n",
    "    min_index = find_min_index(total_payoffs)\n",
    "    rounds_list.append(first_payoffs)\n",
    "    totals_by_round.append([total_payoffs[i] for i in range(num_actions)])\n",
    "    \n",
    "    for i in range(num_rounds - 1):\n",
    "        #print(\"initial total payoffs\", total_payoffs)\n",
    "        #new_payoffs = sorted([round(random.random(),2) for i in range(num_actions)])\n",
    "        new_payoff = round(random.random(), 2)\n",
    "        adversarial_payoffs = [0 for i in range(num_actions)]\n",
    "        #for i in range(num_actions):\n",
    "        #   adversarial_payoffs[total_indexes[i]] = new_payoffs[i]\n",
    "        adversarial_payoffs[min_index] = new_payoff\n",
    "        for i in range(num_actions):\n",
    "            #print(\"t\", total_payoffs[i], \"a\", adversarial_payoffs[i])\n",
    "            total_payoffs[i] += adversarial_payoffs[i]\n",
    "            total_payoffs[i] = round(total_payoffs[i], 2)\n",
    "        \n",
    "        #total_indexes = sorted_indexes(total_payoffs)\n",
    "        min_index = find_min_index(total_payoffs)\n",
    "        new_totals = [total_payoffs[i] for i in range(num_actions)]\n",
    "        totals_by_round.append(new_totals)\n",
    "        #print(new_totals)\n",
    "        rounds_list.append(adversarial_payoffs)\n",
    "\n",
    "    #print(\"utility at each round: \\n\", rounds_list)\n",
    "    #print(\"totals by round: \\n\", totals_by_round)\n",
    "    #print(\"final payoffs: \\n\", total_payoffs)\n",
    "    return rounds_list, totals_by_round\n",
    "\n",
    "\n",
    "#generate_adversarial_payoffs(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e786cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#when generating the bernoulli payoffs, generate the payoffs of each action at each round and the \n",
    "#total payoffs up to that point for each action. i.e. list of lists of payoffs/round & list of lists of aggregated payoffs.\n",
    "#uncomment the last line of the generate_adversarial_payoffs section for an example\n",
    "def find_payoff(success_chance):\n",
    "    comparison_val = random.random()\n",
    "    return int(success_chance > comparison_val)\n",
    "\n",
    "def generate_bernoulli_payoffs(num_actions, num_rounds):\n",
    "    rounds_list = []\n",
    "    totals_by_round = []\n",
    "    total_payoffs = [0 for i in range(num_actions)]\n",
    "    totals_by_round = []\n",
    "    \n",
    "    #generate probability of action success in range (0, 1/2)\n",
    "    action_success_chances = [round(random.random() / 2,2) for i in range(num_actions)]\n",
    "    #print('action success chance', action_success_chances)\n",
    "    \n",
    "    for i in range(num_rounds):\n",
    "        #print(\"initial total payoffs\", total_payoffs)\n",
    "        new_payoffs = [find_payoff(action_success_chances[j]) for j in range(num_actions)]\n",
    "        #print(new_payoffs)\n",
    "        \n",
    "        for i in range(num_actions):\n",
    "            #print(\"t\", total_payoffs[i], \"a\", adversarial_payoffs[i])\n",
    "            total_payoffs[i] += new_payoffs[i]\n",
    "            total_payoffs[i] = round(total_payoffs[i], 2)\n",
    "        \n",
    "        new_totals = [total_payoffs[i] for i in range(num_actions)]\n",
    "        totals_by_round.append(new_totals)\n",
    "        #print(new_totals)\n",
    "        rounds_list.append(new_payoffs)\n",
    "\n",
    "    #print(\"utility at each round: \\n\", rounds_list)\n",
    "    #print(\"totals by round: \\n\", totals_by_round)\n",
    "    #print(\"final payoffs: \\n\", total_payoffs)\n",
    "    return rounds_list, totals_by_round\n",
    "\n",
    "#generate_bernoulli_payoffs(3, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f9448fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_action_payoffs(action_payoffs):\n",
    "    copy_payoffs = [action_payoffs[i] for i in range(len(action_payoffs))]\n",
    "    for i in range(0, len(action_payoffs)):\n",
    "        action_payoffs[i] = copy_payoffs[i-1]\n",
    "    return action_payoffs\n",
    "\n",
    "def generate_rotational_random_payoffs(num_actions, num_rounds):\n",
    "    rounds_list = []\n",
    "    totals_by_round = []\n",
    "    action_payoffs = [round(random.random(), 2) for i in range(num_actions)]\n",
    "    total_payoffs = [0 for i in range(num_actions)]\n",
    "    \n",
    "    for i in range(num_rounds):\n",
    "        action_payoffs = rotate_action_payoffs(action_payoffs)\n",
    "            \n",
    "        for i in range(num_actions):\n",
    "            total_payoffs[i] += action_payoffs[i]\n",
    "            total_payoffs[i] = round(total_payoffs[i], 2)\n",
    "        print(total_payoffs)   \n",
    "        new_totals = [total_payoffs[i] for i in range(num_actions)]    \n",
    "        rounds_list.append([action_payoffs[i] for i in range(num_actions)])\n",
    "        totals_by_round.append(new_totals)\n",
    "        \n",
    "    return rounds_list, totals_by_round\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9560f",
   "metadata": {},
   "source": [
    "Simulate Algorithm Behavior Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f56e3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_random_guessing(rounds_list, totals_by_round, max_payoff):\n",
    "    return simulate_exponential_weights(rounds_list, totals_by_round, 0, max_payoff)\n",
    "\n",
    "\n",
    "def simulate_follow_leader(round, inputs):\n",
    "    #may need to implement this manually.\n",
    "    return simulate_exponential_weights(rounds_list, totals_by_round, 100000, max_payoff)\n",
    "\n",
    "def simulate_exponential_weights(rounds_list, totals_by_round, epsilon, max_payoff):\n",
    "    num_rounds = len(rounds_list)\n",
    "    num_actions = len(rounds_list[0])\n",
    "    choices_made = []\n",
    "    action_weights = []\n",
    "    action_probabilities = [(1/num_actions) for i in range(num_actions)]\n",
    "    current_weights = [1 for i in range(num_actions)]\n",
    "    action_weights.append(current_weights)\n",
    "    alg_payoffs = []\n",
    "    opt_payoffs = []\n",
    "    \n",
    "    for round in range(1, num_rounds):\n",
    "        last_round = round - 1\n",
    "        current_weights = [None for i in range(num_actions)]\n",
    "        for action in range(num_actions):\n",
    "            V_last = totals_by_round[last_round][action]\n",
    "            exp = V_last / max_payoff\n",
    "            current_weights[action] = pow(1 + epsilon, exp)\n",
    "        #randomly select from actions using weights as probabilities\n",
    "        selected_payoff = random.choices(rounds_list[round], weights=current_weights, k=1)[0]\n",
    "        #print('current weights', current_weights)\n",
    "        alg_payoffs.append(selected_payoff)  \n",
    "        opt_payoffs.append(max(rounds_list[round]))\n",
    "        action_weights.append(current_weights)\n",
    "        \n",
    "    return alg_payoffs, totals_by_round\n",
    "\n",
    "#rounds_list_1, payoff_totals_1 = generate_adversarial_payoffs(3, 10)\n",
    "#epsilon_1 = 1\n",
    "#simulate_exponential_weights(rounds_list_1, payoff_totals_1, epsilon_1, max_payoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382165f",
   "metadata": {},
   "source": [
    "Monte Carlo Trials\n",
    "\n",
    "- Declare size of inputs\n",
    "- Generate payoffs\n",
    "- For each learning rate $\\{\\epsilon_1, . . ., \\epsilon_n\\}$\n",
    "    - For each input\n",
    "        - Simulate the algorithm\n",
    "        - calculate OPT (best in hindsight payoff)\n",
    "        - calculate the algorithm's regret\n",
    "    - Calculate the average regret for this learning rate $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "840628cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ALG regret for epsilon = 0 on adversarial distribution = 0.002186464646464645\n",
      "Average ALG regret for epsilon = 0.25 on adversarial distribution = 0.008135050505050482\n",
      "Average ALG regret for epsilon = 0.5 on adversarial distribution = 0.012927171717171723\n",
      "Average ALG regret for epsilon = 0.75 on adversarial distribution = 0.016910909090909067\n",
      "Average ALG regret for epsilon = 1 on adversarial distribution = 0.01996373737373734\n",
      "Average ALG payoff for epsilon = 0 on adversarial distribution = 9.921530000000006\n",
      "Average ALG payoff for epsilon = 0.25 on adversarial distribution = 9.303520000000008\n",
      "Average ALG payoff for epsilon = 0.5 on adversarial distribution = 8.828859999999992\n",
      "Average ALG payoff for epsilon = 0.75 on adversarial distribution = 8.429669999999991\n",
      "Average ALG payoff for epsilon = 1 on adversarial distribution = 8.142370000000001\n",
      "Average OPT payoff for adversarial distribution = 10.342480000000002\n",
      "Average ALG regret for epsilon = 0 on bernoulli distribution = 0.17996969696969928\n",
      "Average ALG regret for epsilon = 0.25 on bernoulli distribution = 0.07319191919192015\n",
      "Average ALG regret for epsilon = 0.5 on bernoulli distribution = 0.05504040404040479\n",
      "Average ALG regret for epsilon = 0.75 on bernoulli distribution = 0.04789898989899054\n",
      "Average ALG regret for epsilon = 1 on bernoulli distribution = 0.046212121212121725\n",
      "Average ALG payoff for epsilon = 0 on bernoulli distribution = 24.549\n",
      "Average ALG payoff for epsilon = 0.25 on bernoulli distribution = 35.305\n",
      "Average ALG payoff for epsilon = 0.5 on bernoulli distribution = 37.085\n",
      "Average ALG payoff for epsilon = 0.75 on bernoulli distribution = 37.808\n",
      "Average ALG payoff for epsilon = 1 on bernoulli distribution = 37.968\n",
      "Average OPT payoff for bernoulli distribution = 42.556\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sum_to_round_i(alg_payoffs, current_round):\n",
    "    total = 0\n",
    "    for i in range(current_round):\n",
    "        total += alg_payoffs[i]\n",
    "    return total\n",
    "\n",
    "def individual_regrets(alg_payoffs, round_totals):\n",
    "    final_payoffs = round_totals[-1]\n",
    "    opt_action = final_payoffs.index(max(final_payoffs))\n",
    "    #print(opt_action)\n",
    "    individual_regrets = [0 for i in range(len(alg_payoffs))]\n",
    "    for round in range((len(alg_payoffs))):\n",
    "        individual_regrets[round] = (round_totals[round][opt_action] - sum_to_round_i(alg_payoffs, round)) / (round + 1)\n",
    "    return individual_regrets\n",
    "    \n",
    "\n",
    "rounds = 100\n",
    "actions = 5\n",
    "N = 1000\n",
    "# ADD OPTIMAL LEARNING RATE EPSILON\n",
    "learning_rates = [0, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "#adversarial monte carlo trial\n",
    "max_payoff = 1\n",
    "avg_lr_payoffs = dict()\n",
    "all_opt_payoffs = []\n",
    "avg_regret_per_round = dict()\n",
    "for n in range(N):\n",
    "    adversarial_payoffs, adversarial_totals = generate_adversarial_payoffs(actions, rounds)\n",
    "    for epsilon in learning_rates:\n",
    "        adv_payoffs, adv_round_totals = simulate_exponential_weights(adversarial_payoffs, adversarial_totals, epsilon, max_payoff)\n",
    "        adv_regrets = individual_regrets(adv_payoffs, adv_round_totals)\n",
    "        adv_avg_regrets = sum(adv_regrets) / len(adv_regrets)\n",
    "        adv_final_regret = adv_regrets[-1]\n",
    "        if epsilon not in avg_regret_per_round:\n",
    "            avg_regret_per_round[epsilon] = adv_regrets\n",
    "        else:\n",
    "            for i in range(len(avg_regret_per_round[epsilon])):\n",
    "                avg_regret_per_round[epsilon][i] = ((n * avg_regret_per_round[epsilon][i]) + adv_regrets[i]) / (n + 1)            \n",
    "        if epsilon not in avg_lr_payoffs:\n",
    "            avg_lr_payoffs[epsilon] = [sum(adv_payoffs)]\n",
    "        else:\n",
    "            avg_lr_payoffs[epsilon].append(sum(adv_payoffs))\n",
    "    all_opt_payoffs.append(max(adv_round_totals[-1]))\n",
    "for key, val in avg_regret_per_round.items():\n",
    "    print(\"Average ALG regret for epsilon =\", key, \"on adversarial distribution =\", val[-1])\n",
    "\n",
    "for key, val in avg_lr_payoffs.items():\n",
    "    print(\"Average ALG payoff for epsilon =\", key, \"on adversarial distribution =\", sum(val) / len(val))\n",
    "print(\"Average OPT payoff for adversarial distribution =\", sum(all_opt_payoffs) / len(all_opt_payoffs))  \n",
    "\n",
    "#bernoulli monte carlo trial\n",
    "max_payoff = 1\n",
    "avg_lr_payoffs = dict()\n",
    "all_opt_payoffs = []\n",
    "avg_regret_per_round = dict()\n",
    "for n in range(N):\n",
    "    bernoulli_payoffs, bernoulli_totals = generate_bernoulli_payoffs(actions, rounds)\n",
    "    for epsilon in learning_rates:\n",
    "        bern_payoffs, bern_round_totals = simulate_exponential_weights(bernoulli_payoffs, bernoulli_totals, epsilon, max_payoff)\n",
    "        bern_regrets = individual_regrets(bern_payoffs, bern_round_totals)\n",
    "        bern_avg_regrets = sum(bern_regrets) / len(bern_regrets)\n",
    "        bern_final_regret = bern_regrets[-1]\n",
    "        if epsilon not in avg_regret_per_round:\n",
    "            avg_regret_per_round[epsilon] = bern_regrets\n",
    "        else:\n",
    "            for i in range(len(avg_regret_per_round[epsilon])):\n",
    "                avg_regret_per_round[epsilon][i] = ((n * avg_regret_per_round[epsilon][i]) + bern_regrets[i]) / (n + 1)\n",
    "        \n",
    "        if epsilon not in avg_lr_payoffs:\n",
    "            avg_lr_payoffs[epsilon] = [sum(bern_payoffs)]\n",
    "        else:\n",
    "            avg_lr_payoffs[epsilon].append(sum(bern_payoffs))\n",
    "    \n",
    "    all_opt_payoffs.append(max(bern_round_totals[-1]))\n",
    "for key, val in avg_regret_per_round.items():\n",
    "    print(\"Average ALG regret for epsilon =\", key, \"on bernoulli distribution =\", val[-1])\n",
    "for key, val in avg_lr_payoffs.items():\n",
    "    print(\"Average ALG payoff for epsilon =\", key, \"on bernoulli distribution =\", sum(val) / len(val))\n",
    "print(\"Average OPT payoff for bernoulli distribution =\", sum(all_opt_payoffs) / len(all_opt_payoffs) )\n",
    "\n",
    "# rotational generation monte carlo trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106a93f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc127d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da96d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
