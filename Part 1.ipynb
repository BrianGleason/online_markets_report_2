{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e4d158",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a543f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "import math\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26912867",
   "metadata": {},
   "source": [
    "## Generate Distribution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e76197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_indexes(payoffs):\n",
    "    vals_indexes=[]\n",
    "    ind_by_val = []\n",
    "\n",
    "    for i in range(len(payoffs)):\n",
    "        vals_indexes.append([payoffs[i],i])\n",
    "    \n",
    "    vals_indexes.sort(reverse=True)\n",
    "    for x in vals_indexes:\n",
    "        ind_by_val.append(x[1])\n",
    "    return ind_by_val\n",
    "\n",
    "def find_min_index(payoffs):\n",
    "    min_value = min(payoffs)\n",
    "    min_index = payoffs.index(min_value)\n",
    "    return min_index\n",
    "\n",
    "\n",
    "def generate_adversarial_payoffs(num_actions, num_rounds):\n",
    "    rounds_list = []\n",
    "    totals_by_round = []\n",
    "    initial_payoff = round(random.random(), 2)\n",
    "    first_payoffs = [0 for i in range(num_actions)]\n",
    "    first_payoffs[random.randrange(num_actions)] = initial_payoff\n",
    "    total_payoffs = [first_payoffs[i] for i in range(num_actions)]\n",
    "    min_index = find_min_index(total_payoffs)\n",
    "    rounds_list.append(first_payoffs)\n",
    "    totals_by_round.append([total_payoffs[i] for i in range(num_actions)])\n",
    "    \n",
    "    for i in range(num_rounds - 1):\n",
    "        new_payoff = round(random.random(), 2)\n",
    "        adversarial_payoffs = [0 for i in range(num_actions)]\n",
    "        adversarial_payoffs[min_index] = new_payoff\n",
    "        for i in range(num_actions):\n",
    "            total_payoffs[i] += adversarial_payoffs[i]\n",
    "            total_payoffs[i] = round(total_payoffs[i], 2)\n",
    "        \n",
    "        min_index = find_min_index(total_payoffs)\n",
    "        new_totals = [total_payoffs[i] for i in range(num_actions)]\n",
    "        totals_by_round.append(new_totals)\n",
    "        rounds_list.append(adversarial_payoffs)\n",
    "\n",
    "    #print(\"utility at each round: \\n\", rounds_list)\n",
    "    #print(\"totals by round: \\n\", totals_by_round)\n",
    "    #print(\"final payoffs: \\n\", total_payoffs)\n",
    "    return rounds_list, totals_by_round\n",
    "\n",
    "\n",
    "#generate_adversarial_payoffs(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e786cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#when generating the bernoulli payoffs, generate the payoffs of each action at each round and the \n",
    "#total payoffs up to that point for each action. i.e. list of lists of payoffs/round & list of lists of aggregated payoffs.\n",
    "#uncomment the last line of the generate_adversarial_payoffs section for an example\n",
    "def find_payoff(success_chance):\n",
    "    comparison_val = random.random()\n",
    "    return int(success_chance > comparison_val)\n",
    "\n",
    "def generate_bernoulli_payoffs(num_actions, num_rounds):\n",
    "    rounds_list = []\n",
    "    totals_by_round = []\n",
    "    total_payoffs = [0 for i in range(num_actions)]\n",
    "    totals_by_round = []\n",
    "    action_success_chances = [round(random.random() / 2,2) for i in range(num_actions)]\n",
    "    \n",
    "    for i in range(num_rounds):\n",
    "        new_payoffs = [find_payoff(action_success_chances[j]) for j in range(num_actions)]\n",
    "        \n",
    "        for i in range(num_actions):\n",
    "            total_payoffs[i] += new_payoffs[i]\n",
    "            total_payoffs[i] = round(total_payoffs[i], 2)\n",
    "        \n",
    "        new_totals = [total_payoffs[i] for i in range(num_actions)]\n",
    "        totals_by_round.append(new_totals)\n",
    "        rounds_list.append(new_payoffs)\n",
    "\n",
    "    #print(\"utility at each round: \\n\", rounds_list)\n",
    "    #print(\"totals by round: \\n\", totals_by_round)\n",
    "    #print(\"final payoffs: \\n\", total_payoffs)\n",
    "    return rounds_list, totals_by_round\n",
    "\n",
    "#generate_bernoulli_payoffs(3, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9448fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_action_payoffs(action_payoffs):\n",
    "    copy_payoffs = [action_payoffs[i] for i in range(len(action_payoffs))]\n",
    "    for i in range(0, len(action_payoffs)):\n",
    "        action_payoffs[i] = copy_payoffs[i-1]\n",
    "    return action_payoffs\n",
    "\n",
    "def generate_rotational_random_payoffs(num_actions, num_rounds):\n",
    "    rounds_list = []\n",
    "    totals_by_round = []\n",
    "    action_payoffs = [round(random.random(), 2) for i in range(num_actions)]\n",
    "    max_index = action_payoffs.index(max(action_payoffs))\n",
    "    secondary_max_index = max_index - 1\n",
    "    max_payoff = action_payoffs[max_index]\n",
    "    if max_index == 0:\n",
    "        action_payoffs[-1] = 0\n",
    "    else:\n",
    "        action_payoffs[max_index - 1] = 0\n",
    "    total_payoffs = [0 for i in range(num_actions)]\n",
    "    action_payoffs = [0 for i in range(num_actions)]\n",
    "    action_payoffs[max_index] = max_payoff\n",
    "    \n",
    "    for i in range(num_rounds):\n",
    "        #if random.random() > 0.9:\n",
    "        #action_payoffs = rotate_action_payoffs(action_payoffs)\n",
    "        action_payoffs[max_index], action_payoffs[secondary_max_index] = action_payoffs[secondary_max_index], action_payoffs[max_index]\n",
    "            \n",
    "        for i in range(num_actions):\n",
    "            total_payoffs[i] += action_payoffs[i]\n",
    "            total_payoffs[i] = round(total_payoffs[i], 2)\n",
    "        new_totals = [total_payoffs[i] for i in range(num_actions)]    \n",
    "        rounds_list.append([action_payoffs[i] for i in range(num_actions)])\n",
    "        totals_by_round.append(new_totals)\n",
    "        \n",
    "    return rounds_list, totals_by_round\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9560f",
   "metadata": {},
   "source": [
    "## Simulate Algorithm Behavior Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_exponential_weights(rounds_list, totals_by_round, epsilon, max_payoff):\n",
    "    num_rounds = len(rounds_list)\n",
    "    num_actions = len(rounds_list[0])\n",
    "    choices_made = []\n",
    "    action_weights = []\n",
    "    action_probabilities = [(1/num_actions) for i in range(num_actions)]\n",
    "    current_weights = [1 for i in range(num_actions)]\n",
    "    action_weights.append(current_weights)\n",
    "    alg_payoffs = []\n",
    "    opt_payoffs = []\n",
    "    \n",
    "    for round in range(1, num_rounds):\n",
    "        last_round = round - 1\n",
    "        current_weights = [None for i in range(num_actions)]\n",
    "        for action in range(num_actions):\n",
    "            V_last = totals_by_round[last_round][action]\n",
    "            exp = V_last / max_payoff\n",
    "            current_weights[action] = pow(1 + epsilon, exp)\n",
    "        #randomly select from actions using weights as probabilities\n",
    "        selected_payoff = random.choices(rounds_list[round], weights=current_weights, k=1)[0]\n",
    "        alg_payoffs.append(selected_payoff)  \n",
    "        opt_payoffs.append(max(rounds_list[round]))\n",
    "        action_weights.append(current_weights)\n",
    "        \n",
    "    return alg_payoffs, totals_by_round, opt_payoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470ce7d6",
   "metadata": {},
   "source": [
    "## Regret Visual Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b6307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_regret(avg_regret_per_round, rounds, learning_rates, plot_title, file_name):\n",
    "    \n",
    "    add_str = ''\n",
    "    for i in range(len(learning_rates)):\n",
    "        if i == 3: \n",
    "            each_lr = round(learning_rates[i], 2)\n",
    "            add_str = '(theor opt learn rate)'\n",
    "        else: \n",
    "            each_lr = learning_rates[i]\n",
    "            add_str = ''\n",
    "        #print(each_lr)\n",
    "        x = np.array(list(range(1, rounds)))\n",
    "        y = np.array(avg_regret_per_round[learning_rates[i]])\n",
    "        plt.plot(x, y, label='learning rate = {each_lr} {add_str}'.format(each_lr=each_lr, add_str = add_str), linewidth=1)\n",
    "    plt.xlabel(\"Round\")\n",
    "    plt.ylabel(\"Regret\")\n",
    "    plt.title(plot_title)\n",
    "    plt.legend(loc='best', prop={'size': 7})\n",
    "    \n",
    "    plt.savefig(file_name)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382165f",
   "metadata": {},
   "source": [
    "## Monte Carlo Trials\n",
    "\n",
    "- Declare size of inputs\n",
    "- Generate payoffs\n",
    "- For each learning rate $\\{\\epsilon_1, . . ., \\epsilon_n\\}$\n",
    "    - For each input\n",
    "        - Simulate the algorithm\n",
    "        - calculate OPT (best in hindsight payoff)\n",
    "        - calculate the algorithm's regret\n",
    "    - Calculate the average regret for this learning rate $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840628cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sum_to_round_i(alg_payoffs, current_round):\n",
    "    total = 0\n",
    "    for i in range(current_round):\n",
    "        total += alg_payoffs[i]\n",
    "    return total\n",
    "\n",
    "def individual_regrets(alg_payoffs, round_totals):\n",
    "    final_payoffs = round_totals[-1]\n",
    "    opt_action = final_payoffs.index(max(final_payoffs))\n",
    "    #print(opt_action)\n",
    "    individual_regrets = [0 for i in range(len(alg_payoffs))]\n",
    "    for round in range((len(alg_payoffs))):\n",
    "        individual_regrets[round] = (round_totals[round][opt_action] - sum_to_round_i(alg_payoffs, round)) / (round + 1)\n",
    "    return individual_regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa585a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 100\n",
    "actions = 5\n",
    "N = 1000\n",
    "# ADD OPTIMAL LEARNING RATE EPSILON\n",
    "opt_lr_eps = math.sqrt(numpy.log(actions)/rounds)\n",
    "learning_rates = [0, 0.25, 0.5, opt_lr_eps, 0.75, 1, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adversarial monte carlo trial\n",
    "max_payoff = 1\n",
    "avg_lr_payoffs = dict()\n",
    "all_opt_payoffs = []\n",
    "avg_regret_per_round = dict()\n",
    "avg_regret_per_round_1 = dict()\n",
    "for n in range(N):\n",
    "    adversarial_payoffs, adversarial_totals = generate_adversarial_payoffs(actions, rounds)\n",
    "    for epsilon in learning_rates:\n",
    "        adv_payoffs, adv_round_totals, adv_opt = simulate_exponential_weights(adversarial_payoffs, adversarial_totals, epsilon, max_payoff)\n",
    "        adv_regrets = individual_regrets(adv_payoffs, adv_round_totals)\n",
    "        adv_avg_regrets = sum(adv_regrets) / len(adv_regrets)\n",
    "        adv_final_regret = adv_regrets[-1]\n",
    "        adv_regrets_1 = []\n",
    "        for i in range(len(adv_payoffs)):\n",
    "            adv_regrets_1.append(adv_opt[i] - adv_payoffs[i])\n",
    "        \n",
    "        if epsilon not in avg_regret_per_round_1:\n",
    "            avg_regret_per_round_1[epsilon] = adv_regrets_1\n",
    "        else:\n",
    "            for i in range(len(avg_regret_per_round_1[epsilon])):\n",
    "                avg_regret_per_round_1[epsilon][i] = ((n * avg_regret_per_round_1[epsilon][i]) + adv_regrets_1[i]) / (n + 1)\n",
    "                \n",
    "        if epsilon not in avg_regret_per_round:\n",
    "            avg_regret_per_round[epsilon] = adv_regrets\n",
    "        else:\n",
    "            for i in range(len(avg_regret_per_round[epsilon])):\n",
    "                avg_regret_per_round[epsilon][i] = ((n * avg_regret_per_round[epsilon][i]) + adv_regrets[i]) / (n + 1)            \n",
    "        \n",
    "        if epsilon not in avg_lr_payoffs:\n",
    "            avg_lr_payoffs[epsilon] = [sum(adv_payoffs)]\n",
    "        else:\n",
    "            avg_lr_payoffs[epsilon].append(sum(adv_payoffs))\n",
    "    all_opt_payoffs.append(max(adv_round_totals[-1]))\n",
    "for key, val in avg_regret_per_round.items():\n",
    "    print(\"Average ALG regret for epsilon =\", key, \"on adversarial distribution =\", val[-1])\n",
    "\n",
    "for key, val in avg_lr_payoffs.items():\n",
    "    print(\"Average ALG payoff for epsilon =\", key, \"on adversarial distribution =\", sum(val) / len(val))\n",
    "print(\"Average OPT payoff for adversarial distribution =\", sum(all_opt_payoffs) / len(all_opt_payoffs))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225508c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_regret(avg_regret_per_round, rounds, learning_rates,'Round of EW Algorithm vs. Regret in Hindsight','adv_plot_1.png')\n",
    "visualize_regret(avg_regret_per_round_1, rounds, learning_rates, 'Round of EW Algorithm vs. Regret on Round-by-Round', 'adv_plot_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b925ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bernoulli monte carlo trial\n",
    "max_payoff = 1\n",
    "avg_lr_payoffs = dict()\n",
    "all_opt_payoffs = []\n",
    "avg_regret_per_round = dict()\n",
    "avg_regret_per_round_1 = dict()\n",
    "\n",
    "for n in range(N):\n",
    "    bernoulli_payoffs, bernoulli_totals = generate_bernoulli_payoffs(actions, rounds)\n",
    "    for epsilon in learning_rates:\n",
    "        bern_payoffs, bern_round_totals, bern_opt = simulate_exponential_weights(bernoulli_payoffs, bernoulli_totals, epsilon, max_payoff)\n",
    "        bern_regrets = individual_regrets(bern_payoffs, bern_round_totals)\n",
    "        bern_avg_regrets = sum(bern_regrets) / len(bern_regrets)\n",
    "        bern_final_regret = bern_regrets[-1]\n",
    "        bern_regrets_1 = []\n",
    "        for i in range(len(adv_payoffs)):\n",
    "            bern_regrets_1.append(bern_opt[i] - bern_payoffs[i])\n",
    "        \n",
    "        if epsilon not in avg_regret_per_round_1:\n",
    "            avg_regret_per_round_1[epsilon] = bern_regrets_1\n",
    "        else:\n",
    "            for i in range(len(avg_regret_per_round_1[epsilon])):\n",
    "                avg_regret_per_round_1[epsilon][i] = ((n * avg_regret_per_round_1[epsilon][i]) + bern_regrets_1[i]) / (n + 1)\n",
    "        \n",
    "        if epsilon not in avg_regret_per_round:\n",
    "            avg_regret_per_round[epsilon] = bern_regrets\n",
    "        else:\n",
    "            for i in range(len(avg_regret_per_round[epsilon])):\n",
    "                avg_regret_per_round[epsilon][i] = ((n * avg_regret_per_round[epsilon][i]) + bern_regrets[i]) / (n + 1)\n",
    "        \n",
    "        if epsilon not in avg_lr_payoffs:\n",
    "            avg_lr_payoffs[epsilon] = [sum(bern_payoffs)]\n",
    "        else:\n",
    "            avg_lr_payoffs[epsilon].append(sum(bern_payoffs))\n",
    "    \n",
    "    all_opt_payoffs.append(max(bern_round_totals[-1]))\n",
    "for key, val in avg_regret_per_round.items():\n",
    "    print(\"Average ALG regret for epsilon =\", key, \"on bernoulli distribution =\", val[-1])\n",
    "for key, val in avg_lr_payoffs.items():\n",
    "    print(\"Average ALG payoff for epsilon =\", key, \"on bernoulli distribution =\", sum(val) / len(val))\n",
    "print(\"Average OPT payoff for bernoulli distribution =\", sum(all_opt_payoffs) / len(all_opt_payoffs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e994c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_regret(avg_regret_per_round, rounds, learning_rates, 'Round of EW Algorithm vs. Regret in Hindsight', 'bern_plot_1.png')\n",
    "visualize_regret(avg_regret_per_round_1, rounds, learning_rates, 'Round of EW Algorithm vs. Regret on Round-by-Round', 'bern_plot_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6094be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotational generation monte carlo trial\n",
    "generate_rotational_random_payoffs\n",
    "max_payoff = 1\n",
    "avg_lr_payoffs = dict()\n",
    "all_opt_payoffs = []\n",
    "avg_regret_per_round = dict()\n",
    "for n in range(N):\n",
    "    rotational_payoffs, rotational_totals = generate_rotational_random_payoffs(actions, rounds)\n",
    "    for epsilon in learning_rates:\n",
    "        rot_payoffs, rot_round_totals, rot_opt = simulate_exponential_weights(rotational_payoffs, rotational_totals, epsilon, max_payoff)\n",
    "        rot_regrets = individual_regrets(rot_payoffs, rot_round_totals)\n",
    "        rot_avg_regrets = sum(rot_regrets) / len(rot_regrets)\n",
    "        rot_final_regret = rot_regrets[-1]\n",
    "        if epsilon not in avg_regret_per_round:\n",
    "            avg_regret_per_round[epsilon] = rot_regrets\n",
    "        else:\n",
    "            for i in range(len(avg_regret_per_round[epsilon])):\n",
    "                avg_regret_per_round[epsilon][i] = ((n * avg_regret_per_round[epsilon][i]) + rot_regrets[i]) / (n + 1)\n",
    "                \n",
    "        rot_regrets_1 = []\n",
    "        for i in range(len(adv_payoffs)):\n",
    "            rot_regrets_1.append(rot_opt[i] - rot_payoffs[i])\n",
    "        \n",
    "        if epsilon not in avg_regret_per_round_1:\n",
    "            avg_regret_per_round_1[epsilon] = rot_regrets_1\n",
    "        else:\n",
    "            for i in range(len(avg_regret_per_round_1[epsilon])):\n",
    "                avg_regret_per_round_1[epsilon][i] = ((n * avg_regret_per_round_1[epsilon][i]) + rot_regrets_1[i]) / (n + 1)\n",
    "        \n",
    "        if epsilon not in avg_lr_payoffs:\n",
    "            avg_lr_payoffs[epsilon] = [sum(rot_payoffs)]\n",
    "        else:\n",
    "            avg_lr_payoffs[epsilon].append(sum(rot_payoffs))\n",
    "    \n",
    "    all_opt_payoffs.append(max(rot_round_totals[-1]))\n",
    "for key, val in avg_regret_per_round.items():\n",
    "    print(\"Average ALG regret for epsilon =\", key, \"on rotational random distribution =\", val[-1])\n",
    "for key, val in avg_lr_payoffs.items():\n",
    "    print(\"Average ALG payoff for epsilon =\", key, \"on rotational random distribution =\", sum(val) / len(val))\n",
    "print(\"Average OPT payoff for rotational random distribution =\", sum(all_opt_payoffs) / len(all_opt_payoffs) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914f4e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_regret(avg_regret_per_round, rounds, learning_rates, 'Round of EW Algorithm vs. Regret in Hindsight', 'rot_plot_1.png')\n",
    "visualize_regret(avg_regret_per_round_1, rounds, learning_rates, 'Round of EW Algorithm vs. Regret on Round-by-Round', 'rot_plot_2.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4104a9cc",
   "metadata": {},
   "source": [
    "# Part 2C. Data in the wild"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228168e8",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stock dataset taken from https://www.kaggle.com/datasets/camnugent/sandp500?select=all_stocks_5yr.csv\n",
    "\n",
    "df = pd.read_csv('all_stocks_5yr.csv') \n",
    "\n",
    "df = df.query('date.str.startswith(\"2017\")', \n",
    "engine=\"python\")\n",
    "\n",
    "# resetting index\n",
    "df.reset_index(inplace = True)\n",
    "\n",
    "df['Day']=df.groupby(['Name']).cumcount()+1\n",
    "\n",
    "#Add payoffs of each stock each day as column \n",
    "\n",
    "df['payoff'] = np.where(df['close'] / df['open'] >= 1, 1, 0)\n",
    "\n",
    "#get the number of total stocks\n",
    "df.drop_duplicates(subset = [\"Name\"]).shape[0]\n",
    "\n",
    "#get the number of total stocks\n",
    "df.drop_duplicates(subset = [\"Name\"]).shape[0]\n",
    "\n",
    "#Max payoff value\n",
    "\n",
    "df['payoff'].max()\n",
    "\n",
    "#query stocks by date or day\n",
    "df[df['Day'] == 1]\n",
    "\n",
    "df\n",
    "\n",
    "#identify stocks that do not have full year stock prices data\n",
    "\n",
    "temp = df[df['Day'] == 1]\n",
    "temp = temp['Name'].tolist()\n",
    "\n",
    "temp_2 = df[df['Day'] == 251]\n",
    "temp_2 = temp_2['Name'].tolist()\n",
    "\n",
    "print('Stocks that do not have data for all dates throughout the year: ', ','.join(set(temp).difference(temp_2)))\n",
    "\n",
    "#drop outlier stocks with not enough data\n",
    "\n",
    "df = df.drop(df[df.Name == 'BHF'].index)\n",
    "df = df.drop(df[df.Name == 'DXC'].index) \n",
    "df = df.drop(df[df.Name == 'HLT'].index) \n",
    "df = df.drop(df[df.Name == 'APTV'].index) \n",
    "df = df.drop(df[df.Name == 'DWDP'].index) \n",
    "df = df.drop(df[df.Name == 'BHGE'].index) \n",
    "\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify range of all stocks starting with A\n",
    "\n",
    "range_ds = df\n",
    "\n",
    "# get the unique values (rows)\n",
    "range_ds = range_ds.drop_duplicates(subset = [\"Name\"])\n",
    "\n",
    "#First 11 stocks starting with A \n",
    "range_ds = range_ds.head(11)\n",
    "\n",
    "range_ds\n",
    "\n",
    "#first stock\n",
    "range_ds.head(1)['Name']\n",
    "\n",
    "#last stock\n",
    "range_ds.iloc[-1]['Name']\n",
    "\n",
    "#index of last day of 10th stock\n",
    "\n",
    "index = range_ds.index[-1]\n",
    "\n",
    "index\n",
    "\n",
    "#slice first 10 stocks \n",
    "\n",
    "df = df[:index]\n",
    "\n",
    "df\n",
    "\n",
    "df.drop_duplicates(subset = [\"Name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609f2d7",
   "metadata": {},
   "source": [
    "## Applying EW algorithm to stock data with payoffs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d85dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stock_payoffs(dataset, num_actions):\n",
    "    rounds_list = []\n",
    "    totals_by_round = []\n",
    "    total_payoffs = [0 for i in range(num_actions)]\n",
    "    day = 1\n",
    "    list_of_dates = []\n",
    "    while day <= 251:\n",
    "        temp_ds = dataset[dataset['Day'] == day]\n",
    "        #print(len(temp_ds['payoff'].tolist()))\n",
    "        if len(temp_ds['payoff'].tolist()) != num_actions: \n",
    "            day += 1\n",
    "            #print(day)\n",
    "            continue\n",
    "        else: \n",
    "            list_of_dates.append(temp_ds['date'].iloc[0])\n",
    "            action_payoffs = temp_ds['payoff'].tolist()\n",
    "            #print(len(action_payoffs),'action payoffs')\n",
    "            #print(len(total_payoffs),'total_payoffs')\n",
    "            #print(day)\n",
    "            for j in range(num_actions): \n",
    "                total_payoffs[j] += action_payoffs[j]\n",
    "            new_totals = [total_payoffs[i] for i in range(num_actions)]    \n",
    "            rounds_list.append([action_payoffs[i] for i in range(num_actions)])\n",
    "            totals_by_round.append(new_totals)\n",
    "            day += 1\n",
    "\n",
    "    return rounds_list, totals_by_round, list_of_dates\n",
    "\n",
    "#generate_stock_payoffs(df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e88d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actions = 10\n",
    "N = 1000\n",
    "rounds = 251\n",
    "\n",
    "#optimal learning rate epsilon 𝜀 = √(ln k / n)\n",
    "opt_lr_eps = math.sqrt(numpy.log(actions)/rounds)\n",
    "\n",
    "learning_rates = [0, 0.25, 0.5, opt_lr_eps, 0.75, 1, 100]\n",
    "\n",
    "max_payoff = 1 #max payoff of all stocks\n",
    "#max_payoff = 11.678064176749078\n",
    "avg_lr_payoffs = dict()\n",
    "all_opt_payoffs = []\n",
    "avg_regret_per_round = dict()\n",
    "avg_regret_per_round_1 = dict()\n",
    "list_of_dates = []\n",
    "for n in range(N):\n",
    "    stock_payoffs, stock_totals, date_list = generate_stock_payoffs(df, actions)\n",
    "    for epsilon in learning_rates:\n",
    "        sto_payoffs, sto_round_totals, sto_opt_payoffs = simulate_exponential_weights(stock_payoffs, stock_totals, epsilon, max_payoff)\n",
    "        sto_regrets = individual_regrets(sto_payoffs, sto_round_totals)\n",
    "        sto_avg_regrets = sum(sto_regrets) / len(sto_regrets)\n",
    "        sto_final_regret = sto_regrets[-1]\n",
    "        if epsilon not in avg_regret_per_round:\n",
    "            avg_regret_per_round[epsilon] = sto_regrets\n",
    "        else:\n",
    "            for i in range(len(avg_regret_per_round[epsilon])):\n",
    "                avg_regret_per_round[epsilon][i] = ((n * avg_regret_per_round[epsilon][i]) + sto_regrets[i]) / (n + 1)\n",
    "                \n",
    "        sto_regrets_1 = []\n",
    "        for i in range(len(sto_payoffs)):\n",
    "            sto_regrets_1.append(sto_opt_payoffs[i] - sto_payoffs[i])\n",
    "        \n",
    "        if epsilon not in avg_regret_per_round_1:\n",
    "            avg_regret_per_round_1[epsilon] = sto_regrets_1\n",
    "        else:\n",
    "            for i in range(len(avg_regret_per_round_1[epsilon])):\n",
    "                avg_regret_per_round_1[epsilon][i] = ((n * avg_regret_per_round_1[epsilon][i]) + sto_regrets_1[i]) / (n + 1)\n",
    "\n",
    "        if epsilon not in avg_lr_payoffs:\n",
    "            avg_lr_payoffs[epsilon] = [sum(sto_payoffs)]\n",
    "        else:\n",
    "            avg_lr_payoffs[epsilon].append(sum(sto_payoffs))\n",
    "    list_of_dates = date_list\n",
    "    \n",
    "all_opt_payoffs.append(max(sto_round_totals[-1]))\n",
    "for key, val in avg_regret_per_round.items():\n",
    "    print(\"Average ALG regret for epsilon =\", key, \"on stock data =\", val[-1])\n",
    "for key, val in avg_lr_payoffs.items():\n",
    "    print(\"Average ALG payoff for epsilon =\", key, \"on stock data =\", sum(val) / len(val))\n",
    "print(\"Average OPT payoff for on stock data =\", sum(all_opt_payoffs) / len(all_opt_payoffs) )\n",
    "print(\"The list of dates where you are investing are the following...\",  list_of_dates)\n",
    "print(\"The number of days that you are investing is \",  len(list_of_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c197064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_regret(avg_regret_per_round, rounds, learning_rates, 'Round of EW Algorithm vs. Average Regret', 'stock_plot.png')\n",
    "visualize_regret(avg_regret_per_round_1, rounds, learning_rates, 'Round of EW Algorithm vs. Average Regret', 'stock_plot.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
