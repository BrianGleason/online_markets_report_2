{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e4d158",
   "metadata": {},
   "source": [
    "Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a543f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26912867",
   "metadata": {},
   "source": [
    "Generate Distribution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08e76197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sorted_indexes(payoffs):\n",
    "    vals_indexes=[]\n",
    "    ind_by_val = []\n",
    "\n",
    "    for i in range(len(payoffs)):\n",
    "        vals_indexes.append([payoffs[i],i])\n",
    "    \n",
    "    vals_indexes.sort(reverse=True)\n",
    "    for x in vals_indexes:\n",
    "        ind_by_val.append(x[1])\n",
    "    return ind_by_val\n",
    "\n",
    "def find_min_index(payoffs):\n",
    "    min_value = min(payoffs)\n",
    "    min_index = payoffs.index(min_value)\n",
    "    return min_index\n",
    "\n",
    "\n",
    "def generate_adversarial_payoffs(num_actions, num_rounds):\n",
    "    rounds_list = []\n",
    "    totals_by_round = []\n",
    "    initial_payoff = round(random.random(), 2)\n",
    "    first_payoffs = [0 for i in range(num_actions)]\n",
    "    first_payoffs[random.randrange(num_actions)] = initial_payoff\n",
    "    total_payoffs = [first_payoffs[i] for i in range(num_actions)]\n",
    "    min_index = find_min_index(total_payoffs)\n",
    "    rounds_list.append(first_payoffs)\n",
    "    totals_by_round.append([total_payoffs[i] for i in range(num_actions)])\n",
    "    \n",
    "    for i in range(num_rounds - 1):\n",
    "        new_payoff = round(random.random(), 2)\n",
    "        adversarial_payoffs = [0 for i in range(num_actions)]\n",
    "        adversarial_payoffs[min_index] = new_payoff\n",
    "        for i in range(num_actions):\n",
    "            total_payoffs[i] += adversarial_payoffs[i]\n",
    "            total_payoffs[i] = round(total_payoffs[i], 2)\n",
    "        \n",
    "        min_index = find_min_index(total_payoffs)\n",
    "        new_totals = [total_payoffs[i] for i in range(num_actions)]\n",
    "        totals_by_round.append(new_totals)\n",
    "        rounds_list.append(adversarial_payoffs)\n",
    "\n",
    "    #print(\"utility at each round: \\n\", rounds_list)\n",
    "    #print(\"totals by round: \\n\", totals_by_round)\n",
    "    #print(\"final payoffs: \\n\", total_payoffs)\n",
    "    return rounds_list, totals_by_round\n",
    "\n",
    "\n",
    "#generate_adversarial_payoffs(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e786cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#when generating the bernoulli payoffs, generate the payoffs of each action at each round and the \n",
    "#total payoffs up to that point for each action. i.e. list of lists of payoffs/round & list of lists of aggregated payoffs.\n",
    "#uncomment the last line of the generate_adversarial_payoffs section for an example\n",
    "def find_payoff(success_chance):\n",
    "    comparison_val = random.random()\n",
    "    return int(success_chance > comparison_val)\n",
    "\n",
    "def generate_bernoulli_payoffs(num_actions, num_rounds):\n",
    "    rounds_list = []\n",
    "    totals_by_round = []\n",
    "    total_payoffs = [0 for i in range(num_actions)]\n",
    "    totals_by_round = []\n",
    "    action_success_chances = [round(random.random() / 2,2) for i in range(num_actions)]\n",
    "    \n",
    "    for i in range(num_rounds):\n",
    "        new_payoffs = [find_payoff(action_success_chances[j]) for j in range(num_actions)]\n",
    "        \n",
    "        for i in range(num_actions):\n",
    "            total_payoffs[i] += new_payoffs[i]\n",
    "            total_payoffs[i] = round(total_payoffs[i], 2)\n",
    "        \n",
    "        new_totals = [total_payoffs[i] for i in range(num_actions)]\n",
    "        totals_by_round.append(new_totals)\n",
    "        rounds_list.append(new_payoffs)\n",
    "\n",
    "    #print(\"utility at each round: \\n\", rounds_list)\n",
    "    #print(\"totals by round: \\n\", totals_by_round)\n",
    "    #print(\"final payoffs: \\n\", total_payoffs)\n",
    "    return rounds_list, totals_by_round\n",
    "\n",
    "#generate_bernoulli_payoffs(3, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f9448fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_action_payoffs(action_payoffs):\n",
    "    copy_payoffs = [action_payoffs[i] for i in range(len(action_payoffs))]\n",
    "    for i in range(0, len(action_payoffs)):\n",
    "        action_payoffs[i] = copy_payoffs[i-1]\n",
    "    return action_payoffs\n",
    "\n",
    "def generate_rotational_random_payoffs(num_actions, num_rounds):\n",
    "    rounds_list = []\n",
    "    totals_by_round = []\n",
    "    action_payoffs = [round(random.random(), 2) for i in range(num_actions)]\n",
    "    total_payoffs = [0 for i in range(num_actions)]\n",
    "    \n",
    "    for i in range(num_rounds):\n",
    "        if random.random() > 0.9:\n",
    "            action_payoffs = rotate_action_payoffs(action_payoffs)\n",
    "            \n",
    "        for i in range(num_actions):\n",
    "            total_payoffs[i] += action_payoffs[i]\n",
    "            total_payoffs[i] = round(total_payoffs[i], 2)\n",
    "        new_totals = [total_payoffs[i] for i in range(num_actions)]    \n",
    "        rounds_list.append([action_payoffs[i] for i in range(num_actions)])\n",
    "        totals_by_round.append(new_totals)\n",
    "        \n",
    "    return rounds_list, totals_by_round\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9560f",
   "metadata": {},
   "source": [
    "Simulate Algorithm Behavior Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f56e3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_exponential_weights(rounds_list, totals_by_round, epsilon, max_payoff):\n",
    "    num_rounds = len(rounds_list)\n",
    "    num_actions = len(rounds_list[0])\n",
    "    choices_made = []\n",
    "    action_weights = []\n",
    "    action_probabilities = [(1/num_actions) for i in range(num_actions)]\n",
    "    current_weights = [1 for i in range(num_actions)]\n",
    "    action_weights.append(current_weights)\n",
    "    alg_payoffs = []\n",
    "    opt_payoffs = []\n",
    "    \n",
    "    for round in range(1, num_rounds):\n",
    "        last_round = round - 1\n",
    "        current_weights = [None for i in range(num_actions)]\n",
    "        for action in range(num_actions):\n",
    "            V_last = totals_by_round[last_round][action]\n",
    "            exp = V_last / max_payoff\n",
    "            current_weights[action] = pow(1 + epsilon, exp)\n",
    "        #randomly select from actions using weights as probabilities\n",
    "        selected_payoff = random.choices(rounds_list[round], weights=current_weights, k=1)[0]\n",
    "        alg_payoffs.append(selected_payoff)  \n",
    "        opt_payoffs.append(max(rounds_list[round]))\n",
    "        action_weights.append(current_weights)\n",
    "        \n",
    "    return alg_payoffs, totals_by_round"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382165f",
   "metadata": {},
   "source": [
    "Monte Carlo Trials\n",
    "\n",
    "- Declare size of inputs\n",
    "- Generate payoffs\n",
    "- For each learning rate $\\{\\epsilon_1, . . ., \\epsilon_n\\}$\n",
    "    - For each input\n",
    "        - Simulate the algorithm\n",
    "        - calculate OPT (best in hindsight payoff)\n",
    "        - calculate the algorithm's regret\n",
    "    - Calculate the average regret for this learning rate $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "840628cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ALG regret for epsilon = 0 on adversarial distribution = 0.001236767676767675\n",
      "Average ALG regret for epsilon = 0.25 on adversarial distribution = 0.008089797979797959\n",
      "Average ALG regret for epsilon = 0.5 on adversarial distribution = 0.012251515151515152\n",
      "Average ALG regret for epsilon = 0.12686362411795196 on adversarial distribution = 0.005375252525252534\n",
      "Average ALG regret for epsilon = 0.75 on adversarial distribution = 0.01618494949494947\n",
      "Average ALG regret for epsilon = 1 on adversarial distribution = 0.020151313131313086\n",
      "Average ALG regret for epsilon = 100 on adversarial distribution = 0.07966555555555568\n",
      "Average ALG payoff for epsilon = 0 on adversarial distribution = 9.969540000000004\n",
      "Average ALG payoff for epsilon = 0.25 on adversarial distribution = 9.29121000000001\n",
      "Average ALG payoff for epsilon = 0.5 on adversarial distribution = 8.859720000000001\n",
      "Average ALG payoff for epsilon = 0.12686362411795196 on adversarial distribution = 9.565109999999994\n",
      "Average ALG payoff for epsilon = 0.75 on adversarial distribution = 8.481040000000014\n",
      "Average ALG payoff for epsilon = 1 on adversarial distribution = 8.078630000000002\n",
      "Average ALG payoff for epsilon = 100 on adversarial distribution = 2.118119999999998\n",
      "Average OPT payoff for adversarial distribution = 10.309869999999986\n",
      "Average ALG regret for epsilon = 0 on bernoulli distribution = 0.1793030303030324\n",
      "Average ALG regret for epsilon = 0.25 on bernoulli distribution = 0.0736262626262636\n",
      "Average ALG regret for epsilon = 0.5 on bernoulli distribution = 0.055131313131313836\n",
      "Average ALG regret for epsilon = 0.12686362411795196 on bernoulli distribution = 0.10050505050505182\n",
      "Average ALG regret for epsilon = 0.75 on bernoulli distribution = 0.049040404040404655\n",
      "Average ALG regret for epsilon = 1 on bernoulli distribution = 0.046313131313131986\n",
      "Average ALG regret for epsilon = 100 on bernoulli distribution = 0.036292929292929606\n",
      "Average ALG payoff for epsilon = 0 on bernoulli distribution = 24.95\n",
      "Average ALG payoff for epsilon = 0.25 on bernoulli distribution = 35.515\n",
      "Average ALG payoff for epsilon = 0.5 on bernoulli distribution = 37.347\n",
      "Average ALG payoff for epsilon = 0.12686362411795196 on bernoulli distribution = 32.841\n",
      "Average ALG payoff for epsilon = 0.75 on bernoulli distribution = 37.948\n",
      "Average ALG payoff for epsilon = 1 on bernoulli distribution = 38.226\n",
      "Average ALG payoff for epsilon = 100 on bernoulli distribution = 39.213\n",
      "Average OPT payoff for bernoulli distribution = 42.825\n",
      "Average ALG regret for epsilon = 0 on rotational random distribution = 0.10162141414141433\n",
      "Average ALG regret for epsilon = 0.25 on rotational random distribution = 0.058169696969697\n",
      "Average ALG regret for epsilon = 0.5 on rotational random distribution = 0.04481737373737372\n",
      "Average ALG regret for epsilon = 0.12686362411795196 on rotational random distribution = 0.07294797979797984\n",
      "Average ALG regret for epsilon = 0.75 on rotational random distribution = 0.037924343434343415\n",
      "Average ALG regret for epsilon = 1 on rotational random distribution = 0.03340404040404043\n",
      "Average ALG regret for epsilon = 100 on rotational random distribution = 0.018502525252525312\n",
      "Average ALG payoff for epsilon = 0 on rotational random distribution = 49.851810000000015\n",
      "Average ALG payoff for epsilon = 0.25 on rotational random distribution = 54.184759999999976\n",
      "Average ALG payoff for epsilon = 0.5 on rotational random distribution = 55.51923999999994\n",
      "Average ALG payoff for epsilon = 0.12686362411795196 on rotational random distribution = 52.72853999999998\n",
      "Average ALG payoff for epsilon = 0.75 on rotational random distribution = 56.204279999999954\n",
      "Average ALG payoff for epsilon = 1 on rotational random distribution = 56.65126000000002\n",
      "Average ALG payoff for epsilon = 100 on rotational random distribution = 58.126799999999925\n",
      "Average OPT payoff for rotational random distribution = 59.9656\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sum_to_round_i(alg_payoffs, current_round):\n",
    "    total = 0\n",
    "    for i in range(current_round):\n",
    "        total += alg_payoffs[i]\n",
    "    return total\n",
    "\n",
    "def individual_regrets(alg_payoffs, round_totals):\n",
    "    final_payoffs = round_totals[-1]\n",
    "    opt_action = final_payoffs.index(max(final_payoffs))\n",
    "    #print(opt_action)\n",
    "    individual_regrets = [0 for i in range(len(alg_payoffs))]\n",
    "    for round in range((len(alg_payoffs))):\n",
    "        individual_regrets[round] = (round_totals[round][opt_action] - sum_to_round_i(alg_payoffs, round)) / (round + 1)\n",
    "    return individual_regrets\n",
    "    \n",
    "\n",
    "rounds = 100\n",
    "actions = 5\n",
    "N = 1000\n",
    "# ADD OPTIMAL LEARNING RATE EPSILON\n",
    "learning_rates = [0, 0.25, 0.5, math.sqrt(numpy.log(actions)/rounds), 0.75, 1, 100]\n",
    "\n",
    "#adversarial monte carlo trial\n",
    "max_payoff = 1\n",
    "avg_lr_payoffs = dict()\n",
    "all_opt_payoffs = []\n",
    "avg_regret_per_round = dict()\n",
    "for n in range(N):\n",
    "    adversarial_payoffs, adversarial_totals = generate_adversarial_payoffs(actions, rounds)\n",
    "    for epsilon in learning_rates:\n",
    "        adv_payoffs, adv_round_totals = simulate_exponential_weights(adversarial_payoffs, adversarial_totals, epsilon, max_payoff)\n",
    "        adv_regrets = individual_regrets(adv_payoffs, adv_round_totals)\n",
    "        adv_avg_regrets = sum(adv_regrets) / len(adv_regrets)\n",
    "        adv_final_regret = adv_regrets[-1]\n",
    "        if epsilon not in avg_regret_per_round:\n",
    "            avg_regret_per_round[epsilon] = adv_regrets\n",
    "        else:\n",
    "            for i in range(len(avg_regret_per_round[epsilon])):\n",
    "                avg_regret_per_round[epsilon][i] = ((n * avg_regret_per_round[epsilon][i]) + adv_regrets[i]) / (n + 1)            \n",
    "        if epsilon not in avg_lr_payoffs:\n",
    "            avg_lr_payoffs[epsilon] = [sum(adv_payoffs)]\n",
    "        else:\n",
    "            avg_lr_payoffs[epsilon].append(sum(adv_payoffs))\n",
    "    all_opt_payoffs.append(max(adv_round_totals[-1]))\n",
    "for key, val in avg_regret_per_round.items():\n",
    "    print(\"Average ALG regret for epsilon =\", key, \"on adversarial distribution =\", val[-1])\n",
    "\n",
    "for key, val in avg_lr_payoffs.items():\n",
    "    print(\"Average ALG payoff for epsilon =\", key, \"on adversarial distribution =\", sum(val) / len(val))\n",
    "print(\"Average OPT payoff for adversarial distribution =\", sum(all_opt_payoffs) / len(all_opt_payoffs))  \n",
    "\n",
    "#bernoulli monte carlo trial\n",
    "max_payoff = 1\n",
    "avg_lr_payoffs = dict()\n",
    "all_opt_payoffs = []\n",
    "avg_regret_per_round = dict()\n",
    "for n in range(N):\n",
    "    bernoulli_payoffs, bernoulli_totals = generate_bernoulli_payoffs(actions, rounds)\n",
    "    for epsilon in learning_rates:\n",
    "        bern_payoffs, bern_round_totals = simulate_exponential_weights(bernoulli_payoffs, bernoulli_totals, epsilon, max_payoff)\n",
    "        bern_regrets = individual_regrets(bern_payoffs, bern_round_totals)\n",
    "        bern_avg_regrets = sum(bern_regrets) / len(bern_regrets)\n",
    "        bern_final_regret = bern_regrets[-1]\n",
    "        if epsilon not in avg_regret_per_round:\n",
    "            avg_regret_per_round[epsilon] = bern_regrets\n",
    "        else:\n",
    "            for i in range(len(avg_regret_per_round[epsilon])):\n",
    "                avg_regret_per_round[epsilon][i] = ((n * avg_regret_per_round[epsilon][i]) + bern_regrets[i]) / (n + 1)\n",
    "        \n",
    "        if epsilon not in avg_lr_payoffs:\n",
    "            avg_lr_payoffs[epsilon] = [sum(bern_payoffs)]\n",
    "        else:\n",
    "            avg_lr_payoffs[epsilon].append(sum(bern_payoffs))\n",
    "    \n",
    "    all_opt_payoffs.append(max(bern_round_totals[-1]))\n",
    "for key, val in avg_regret_per_round.items():\n",
    "    print(\"Average ALG regret for epsilon =\", key, \"on bernoulli distribution =\", val[-1])\n",
    "for key, val in avg_lr_payoffs.items():\n",
    "    print(\"Average ALG payoff for epsilon =\", key, \"on bernoulli distribution =\", sum(val) / len(val))\n",
    "print(\"Average OPT payoff for bernoulli distribution =\", sum(all_opt_payoffs) / len(all_opt_payoffs) )\n",
    "\n",
    "# rotational generation monte carlo trial\n",
    "generate_rotational_random_payoffs\n",
    "max_payoff = 1\n",
    "avg_lr_payoffs = dict()\n",
    "all_opt_payoffs = []\n",
    "avg_regret_per_round = dict()\n",
    "for n in range(N):\n",
    "    rotational_payoffs, rotational_totals = generate_rotational_random_payoffs(actions, rounds)\n",
    "    for epsilon in learning_rates:\n",
    "        rot_payoffs, rot_round_totals = simulate_exponential_weights(rotational_payoffs, rotational_totals, epsilon, max_payoff)\n",
    "        rot_regrets = individual_regrets(rot_payoffs, rot_round_totals)\n",
    "        rot_avg_regrets = sum(rot_regrets) / len(rot_regrets)\n",
    "        rot_final_regret = rot_regrets[-1]\n",
    "        if epsilon not in avg_regret_per_round:\n",
    "            avg_regret_per_round[epsilon] = rot_regrets\n",
    "        else:\n",
    "            for i in range(len(avg_regret_per_round[epsilon])):\n",
    "                avg_regret_per_round[epsilon][i] = ((n * avg_regret_per_round[epsilon][i]) + rot_regrets[i]) / (n + 1)\n",
    "        \n",
    "        if epsilon not in avg_lr_payoffs:\n",
    "            avg_lr_payoffs[epsilon] = [sum(rot_payoffs)]\n",
    "        else:\n",
    "            avg_lr_payoffs[epsilon].append(sum(rot_payoffs))\n",
    "    \n",
    "    all_opt_payoffs.append(max(rot_round_totals[-1]))\n",
    "for key, val in avg_regret_per_round.items():\n",
    "    print(\"Average ALG regret for epsilon =\", key, \"on rotational random distribution =\", val[-1])\n",
    "for key, val in avg_lr_payoffs.items():\n",
    "    print(\"Average ALG payoff for epsilon =\", key, \"on rotational random distribution =\", sum(val) / len(val))\n",
    "print(\"Average OPT payoff for rotational random distribution =\", sum(all_opt_payoffs) / len(all_opt_payoffs) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106a93f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc127d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da96d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
